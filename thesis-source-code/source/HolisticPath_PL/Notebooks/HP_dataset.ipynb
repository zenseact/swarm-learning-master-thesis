{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11c2040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2271751/3977172901.py:42: DeprecationWarning: The 'zod.data_classes.oxts' module has been deprecated and will be removed in a future version. Please use the 'zod.data_classes.ego_motion' module instead.\n",
      "  from zod.data_classes.oxts import EgoMotion\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchsummary import summary\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from numba import cuda\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import enum\n",
    "import threading\n",
    "from matplotlib import pyplot as plt\n",
    "from zod import ZodFrames\n",
    "from zod import ZodSequences\n",
    "import zod.constants as constants\n",
    "from zod.constants import Camera, Lidar, Anonymization, AnnotationProject\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import cv2\n",
    "from flask import Flask, request, jsonify\n",
    "import multiprocessing\n",
    "from zod.visualization.oxts_on_image import visualize_oxts_on_image\n",
    "from zod.constants import Camera\n",
    "from zod.data_classes.calibration import Calibration\n",
    "from zod.data_classes.oxts import EgoMotion\n",
    "from zod.utils.polygon_transformations import polygons_to_binary_mask\n",
    "from zod.utils.geometry import (\n",
    "    get_points_in_camera_fov,\n",
    "    project_3d_to_2d_kannala,\n",
    "    transform_points,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8113a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2271751/1285201340.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 64em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 64em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7763a",
   "metadata": {},
   "source": [
    "## static params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb08e1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch=2.0.0+cu117. Pytorch vision=0.15.1+cu117. Flower=1.3.0\n",
      "Training will run on: cudas\n"
     ]
    }
   ],
   "source": [
    "NUM_OUTPUT = 51\n",
    "IMG_SIZE = 256\n",
    "RUN_PRETRAINED = True\n",
    "BATCH_SIZE = 8\n",
    "VAL_FACTOR = 0.15\n",
    "SUBSET_FACTOR = 0.03\n",
    "USE_GPU = True\n",
    "NUM_GLOBAL_ROUNDS = 3\n",
    "NUM_LOCAL_EPOCHS = 3\n",
    "PRINT_DEBUG_DATA = True\n",
    "NUM_WORKERS = 4 # os.cpu_count()\n",
    "FRAMES_IMAGE_MEAN = [0.337, 0.345, 0.367]\n",
    "FRAMES_IMAGE_STD = [0.160, 0.180, 0.214]\n",
    "DEVICE = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "TRAIN_FRAMES_PATH = \"../GroundTruth/training_seg_annotated_frames.json\"\n",
    "VAL_FRAMES_PATH = \"../GroundTruth/validation_seg_annotated_frames.json\"\n",
    "STORED_GROUND_TRUTH_PATH = \"cached_gt/hp_gt_smp.json\"\n",
    "STORED_BALANCED_DS_PATH = \"cached_gt/balanced_frames.txt\"\n",
    "DATASET_ROOT = \"/mnt/ZOD\"\n",
    "ZENSEACT_DATASET_ROOT = \"/staging/dataset_donation/round_2\"\n",
    "\n",
    "TARGET_DISTANCES = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 95, 110, 125, 145, 165]\n",
    "\n",
    "with open(\"frames_with_less_than_165m_hp.json\") as f:\n",
    "    short_frames = json.load(f)\n",
    "UNUSED_FRAMES = set(short_frames)\n",
    "\n",
    "print(f\"PyTorch={torch.__version__}. Pytorch vision={torchvision.__version__}. Flower={fl.__version__}\")\n",
    "print(f\"Training will run on: {DEVICE}s\")\n",
    "\n",
    "\"\"\" path to tensor board persistent folders\"\"\"\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "TB_PATH = f\"TensorBoard/runs{now}\"\n",
    "TB_CENTRALIZED_SUB_PATH = \"TensorBoard_Centralized/loss\"\n",
    "TB_FEDERATED_SUB_PATH = \"TensorBoard_Federated/loss\"\n",
    "TB_SWARM_SUB_PATH = \"TensorBoard_Swarm/loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4eaf03",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d5ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZODImporter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=DATASET_ROOT,\n",
    "        subset_factor=SUBSET_FACTOR,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        tb_path=TB_PATH,\n",
    "        zod_frames=None,\n",
    "        training_frames=None, \n",
    "        validation_frames=None\n",
    "    ):\n",
    "        if(zod_frames == None):\n",
    "            self.zod_frames = ZodFrames(dataset_root=root, version='full')\n",
    "            self.training_frames_all = self.zod_frames.get_split(constants.TRAIN)\n",
    "            self.validation_frames_all = self.zod_frames.get_split(constants.VAL)\n",
    "            \n",
    "            self.training_frames = list(self.training_frames_all)[: int(len(self.training_frames_all) * subset_factor)]\n",
    "            self.validation_frames = list(self.validation_frames_all)[: int(len(self.validation_frames_all) * subset_factor)]\n",
    "\n",
    "            self.training_frames = [x for x in tqdm(self.training_frames) if self.is_valid(x)]\n",
    "            self.validation_frames = [x for x in tqdm(self.validation_frames) if self.is_valid(x)]\n",
    "        else:\n",
    "            self.zod_frames = zod_frames\n",
    "            self.training_frames = training_frames\n",
    "            self.validation_frames = validation_frames\n",
    "            \n",
    "        print(\"length of training_frames subset:\", len(self.training_frames))\n",
    "        print(\"length of test_frames subset:\", len(self.validation_frames))\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tb_path = tb_path\n",
    "\n",
    "        \n",
    "    def is_valid(self, frame_id):\n",
    "        return frame_id not in UNUSED_FRAMES\n",
    "        \n",
    "    def load_datasets(self, num_clients: int):\n",
    "        seed = 42\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(FRAMES_IMAGE_MEAN, FRAMES_IMAGE_STD),\n",
    "            transforms.Resize(size=(self.img_size, self.img_size), antialias=True)\n",
    "        ])\n",
    "\n",
    "        trainset = ZodDataset(zod_frames=self.zod_frames, frames_id_set=self.training_frames, transform=transform)\n",
    "        testset = ZodDataset(zod_frames=self.zod_frames, frames_id_set=self.validation_frames, transform=transform)\n",
    "\n",
    "        # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "        partition_size = len(trainset) // num_clients\n",
    "\n",
    "        lengths = [partition_size]\n",
    "        if num_clients > 1:\n",
    "            lengths = [partition_size] * (num_clients - 1)\n",
    "            lengths.append(len(trainset) - sum(lengths))\n",
    "\n",
    "        datasets = random_split(trainset, lengths, torch.Generator().manual_seed(seed))\n",
    "\n",
    "        # Split each partition into train/val and create DataLoader\n",
    "        trainloaders, valloaders = [], []\n",
    "        lengths_train, lengths_val = [], []\n",
    "        for ds in datasets:\n",
    "            len_val = int(len(ds) * VAL_FACTOR)\n",
    "            len_train = int(len(ds) - len_val)\n",
    "            lengths_train.append(len_train)\n",
    "            lengths_val.append(len_val)\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(seed))\n",
    "            trainloaders.append(DataLoader(ds_train,batch_size=self.batch_size, shuffle=True, num_workers=NUM_WORKERS))\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=self.batch_size, num_workers=NUM_WORKERS))\n",
    "\n",
    "        len_complete_val = int(len(trainset) * VAL_FACTOR)\n",
    "        len_complete_train = int(len(trainset) - len_complete_val)\n",
    "        train_split, val_split = random_split(\n",
    "            trainset,\n",
    "            [len_complete_train, len_complete_val],\n",
    "            torch.Generator().manual_seed(seed),\n",
    "        )\n",
    "\n",
    "        completeTrainloader = DataLoader(train_split, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n",
    "        completeValloader = DataLoader(val_split, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "        testloader = DataLoader(testset, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "        \"\"\"report to tensor board\"\"\"\n",
    "        save_dataset_tb_plot(self.tb_path, lengths_train, \"training\", seed)\n",
    "        save_dataset_tb_plot(self.tb_path, lengths_val, \"validation\", seed)\n",
    "\n",
    "        return (\n",
    "            trainloaders,\n",
    "            valloaders,\n",
    "            testloader,\n",
    "            completeTrainloader,\n",
    "            completeValloader,\n",
    "        )\n",
    "\n",
    "class ZodDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        zod_frames,\n",
    "        frames_id_set,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "        self.zod_frames = zod_frames\n",
    "        self.frames_id_set = frames_id_set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames_id_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # load frame\n",
    "        frame_idx = self.frames_id_set[idx]\n",
    "        frame = self.zod_frames[frame_idx]\n",
    "        \n",
    "        # get image\n",
    "        image_path = frame.info.get_key_camera_frame(Anonymization.DNAT).filepath\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        # extract ground truth\n",
    "        label = get_ground_truth(self.zod_frames, frame_idx)\n",
    "        \n",
    "        # create sample\n",
    "        sample = dict(image=image, label=label)\n",
    "        \n",
    "        # resize images\n",
    "        image = np.array(Image.fromarray(sample[\"image\"]).resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR))\n",
    "\n",
    "        # convert to other format HWC -> CHW\n",
    "        sample[\"image\"] = np.moveaxis(image, -1, 0)\n",
    "        sample[\"label\"] = np.expand_dims(label, 0)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def get_ground_truth(zod_frames, frame_id):\n",
    "    # get frame\n",
    "    zod_frame = zod_frames[frame_id]\n",
    "\n",
    "    # extract oxts\n",
    "    oxts = zod_frame.oxts\n",
    "\n",
    "    # get timestamp\n",
    "    key_timestamp = zod_frame.info.keyframe_time.timestamp()\n",
    "\n",
    "    try:\n",
    "        # get posses associated with frame timestamp\n",
    "        current_pose = oxts.get_poses(key_timestamp)\n",
    "\n",
    "        # transform poses\n",
    "        all_poses = oxts.poses\n",
    "        transformed_poses = np.linalg.pinv(current_pose) @ all_poses\n",
    "\n",
    "        # get translations\n",
    "        translations = transformed_poses[:, :3, 3]\n",
    "\n",
    "        # calculate acc diff distance\n",
    "        distances = np.linalg.norm(np.diff(translations, axis=0), axis=1)\n",
    "        accumulated_distances = np.cumsum(distances).astype(int).tolist()\n",
    "\n",
    "        # get the poses that each have a point having a distance from TARGET_DISTANCES\n",
    "        pose_idx = [accumulated_distances.index(i) for i in TARGET_DISTANCES]\n",
    "        used_poses = transformed_poses[pose_idx]\n",
    "\n",
    "    except:\n",
    "        #print(\"detected invalid frame: \", frame_id)\n",
    "        return np.array([])\n",
    "\n",
    "    #print(used_poses.shape)\n",
    "    points = used_poses[:, :3, -1]\n",
    "    return points.flatten()\n",
    "\n",
    "def save_dataset_tb_plot(tb_path, sample_distribution, subtitle, seed):\n",
    "    plt.bar(list(range(1, len(sample_distribution) + 1)), sample_distribution)\n",
    "    plt.xlabel(\"Partitions\")\n",
    "    plt.ylabel(\"Samples\")\n",
    "    plt.suptitle(\"Distribution of samples\")\n",
    "    plt.title(\"%s, seed: %s\" % (subtitle, seed)),\n",
    "\n",
    "    \"\"\"report to tensor board\"\"\"\n",
    "    writer = SummaryWriter(tb_path)\n",
    "    writer.add_figure(\"sample_distribution/%s\" % (subtitle), plt.gcf(), global_step=0)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def reshape_ground_truth(label, output_size=NUM_OUTPUT):\n",
    "    return label.reshape(((NUM_OUTPUT // 3), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc97ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d7d8eb18d64c4cbe27b731fd16c4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading infos: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_HP(dataset_root):\n",
    "    zod_frames = ZodFrames(dataset_root=dataset_root, version='full')\n",
    "    training_frames_all = zod_frames.get_split(constants.TRAIN)\n",
    "    validation_frames_all = zod_frames.get_split(constants.VAL)\n",
    "\n",
    "    return zod_frames, training_frames_all, validation_frames_all\n",
    "\n",
    "def is_valid(frame_id):\n",
    "    return frame_id not in UNUSED_FRAMES\n",
    "    \n",
    "zod_frames, training_frames_all, validation_frames_all = load_HP(DATASET_ROOT)\n",
    "\n",
    "training_frames = list(training_frames_all)[: int(len(training_frames_all) * SUBSET_FACTOR)]\n",
    "validation_frames = list(validation_frames_all)[: int(len(validation_frames_all) * SUBSET_FACTOR)]\n",
    "\n",
    "training_frames = [x for x in tqdm(training_frames) if is_valid(x)]\n",
    "validation_frames = [x for x in tqdm(validation_frames) if is_valid(x)]\n",
    "\n",
    "print(f'loaded {len(training_frames)} train frame ids.')\n",
    "print(f'loaded {len(validation_frames)} val frame ids.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4509d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_HP_on_image(zod_frames, frame_id, preds=None, showImg=True):\n",
    "    \"\"\"Visualize oxts track on image plane.\"\"\"\n",
    "    camera=Camera.FRONT\n",
    "    zod_frame = zod_frames[frame_id]\n",
    "    image = zod_frame.get_image(Anonymization.DNAT)\n",
    "    calibs = zod_frame.calibration\n",
    "    points_gt = get_ground_truth(zod_frames, frame_id)\n",
    "    points = reshape_ground_truth(points_gt)\n",
    "    \n",
    "    preds_row = None\n",
    "    if(preds is not None):\n",
    "        preds_row = preds.copy()\n",
    "    \n",
    "    circle_size = 15\n",
    "    \n",
    "    # transform point to camera coordinate system\n",
    "    T_inv = np.linalg.pinv(calibs.get_extrinsics(camera).transform)\n",
    "    camerapoints = transform_points(points[:, :3], T_inv)\n",
    "\n",
    "    # filter points that are not in the camera field of view\n",
    "    points_in_fov = get_points_in_camera_fov(calibs.cameras[camera].field_of_view, camerapoints)\n",
    "    points_in_fov = points_in_fov[0]\n",
    "\n",
    "    # project points to image plane\n",
    "    xy_array = project_3d_to_2d_kannala(\n",
    "        points_in_fov,\n",
    "        calibs.cameras[camera].intrinsics[..., :3],\n",
    "        calibs.cameras[camera].distortion,\n",
    "    )\n",
    "    \n",
    "    ground_truth_color = (19, 80, 41)\n",
    "    preds_color = (161, 65, 137)\n",
    "    \n",
    "    points = []\n",
    "    for i in range(xy_array.shape[0]):\n",
    "        x, y = int(xy_array[i, 0]), int(xy_array[i, 1])\n",
    "        cv2.circle(image, (x,y), circle_size, ground_truth_color, -1)\n",
    "        points.append([x,y])\n",
    "    \n",
    "    \"\"\"Draw a line in image.\"\"\"\n",
    "    def draw_line(image, line, color):\n",
    "        return cv2.polylines(image.copy(), [np.round(line).astype(np.int32)], isClosed=False, color=color, thickness=20)\n",
    "    \n",
    "    image = draw_line(image, points, ground_truth_color)\n",
    "    \n",
    "    # transform and draw predictions \n",
    "    if(preds is not None):\n",
    "        preds = reshape_ground_truth(preds)\n",
    "        predpoints = transform_points(preds[:, :3], T_inv)\n",
    "        predpoints_in_fov = get_points_in_camera_fov(calibs.cameras[camera].field_of_view, predpoints)\n",
    "        predpoints_in_fov = predpoints_in_fov[0]\n",
    "        \n",
    "        xy_array_preds = project_3d_to_2d_kannala(\n",
    "            predpoints_in_fov,\n",
    "            calibs.cameras[camera].intrinsics[..., :3],\n",
    "            calibs.cameras[camera].distortion,\n",
    "        )\n",
    "        preds = []\n",
    "        for i in range(xy_array_preds.shape[0]):\n",
    "            x, y = int(xy_array_preds[i, 0]), int(xy_array_preds[i, 1])\n",
    "            cv2.circle(image, (x,y), circle_size, preds_color, -1)\n",
    "            preds.append([x,y])\n",
    "        \n",
    "        #preds = preds[:(len(preds)//2)]\n",
    "        image = draw_line(image, preds, preds_color)\n",
    "        \n",
    "    #plt.imsave(f'inference_{frame_id}.png', image)\n",
    "    if(showImg):\n",
    "        plt.clf()\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(frame_id)\n",
    "    return image, points_gt, preds_row\n",
    "    \n",
    "def get_transformed_image(zod_frames, frame_id):\n",
    "    frame = zod_frames[frame_id]\n",
    "    image_path = frame.info.get_key_camera_frame(Anonymization.DNAT).filepath\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    image = np.array(Image.fromarray(image).resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR))\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "    print(image.shape)\n",
    "    return image\n",
    "\n",
    "def predict(model, zod_frames, frame_id):\n",
    "    image = get_transformed_image(zod_frames, frame_id).to(DEVICE)\n",
    "    outputs = model(image)\n",
    "    preds = outputs.cpu().detach().numpy()\n",
    "    return preds\n",
    "\n",
    "def visualize_multiple(zod_frames, frame_ids, model_path=None):\n",
    "    if(model_path):\n",
    "        images = [visualize_HP_on_image(zod_frames, frame_id, predict(model_path, zod_frames, frame_id), showImg=False) for frame_id in frame_ids]\n",
    "    else:\n",
    "        images = [visualize_HP_on_image(zod_frames, frame_id, None, showImg=False) for frame_id in frame_ids]\n",
    "        \n",
    "    plt.figure(figsize=(60,60))\n",
    "    columns = 4\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(len(images) // columns + 1, columns, i + 1)\n",
    "        plt.gca().set_title(frame_ids[i])\n",
    "        plt.imshow(image[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd575a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = '075330'\n",
    "_, _, _ = visualize_HP_on_image(zod_frames, frame_id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ids = training_frames[0:20]\n",
    "print(frame_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8197aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_multiple(zod_frames, frame_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
