{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11c2040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_951494/2535533557.py:42: DeprecationWarning: The 'zod.data_classes.oxts' module has been deprecated and will be removed in a future version. Please use the 'zod.data_classes.ego_motion' module instead.\n",
      "  from zod.data_classes.oxts import EgoMotion\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchsummary import summary\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from numba import cuda\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import enum\n",
    "import threading\n",
    "from matplotlib import pyplot as plt\n",
    "from zod import ZodFrames\n",
    "from zod import ZodSequences\n",
    "import zod.constants as constants\n",
    "from zod.constants import Camera, Lidar, Anonymization, AnnotationProject\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import cv2\n",
    "from flask import Flask, request, jsonify\n",
    "import multiprocessing\n",
    "from zod.visualization.oxts_on_image import visualize_oxts_on_image\n",
    "from zod.constants import Camera\n",
    "from zod.data_classes.calibration import Calibration\n",
    "from zod.data_classes.oxts import EgoMotion\n",
    "from zod.utils.polygon_transformations import polygons_to_binary_mask\n",
    "from zod.utils.geometry import (\n",
    "    get_points_in_camera_fov,\n",
    "    project_3d_to_2d_kannala,\n",
    "    transform_points,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "from ema_pytorch import EMA\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8113a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/yasan/miniconda3/envs/zen/lib/python3.9/site-packages/ipykernel_launcher.py', '--ip=127.0.0.1', '--stdin=9016', '--control=9014', '--hb=9013', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"d9bef1ea-7034-4455-b820-1b63e9089596\"', '--shell=9015', '--transport=\"tcp\"', '--iopub=9017', '--f=/home/yasan/.local/share/jupyter/runtime/kernel-v2-4147035uyByFrpxEMe7.json']\n",
      "{'exp_id': 3, 'type': 'federated', 'agent_id': 0, 'output_size': 51, 'image_size': 256, 'run_pretrained': True, 'batch_size': 8, 'val_factor': 0.1, 'subset_factor': 0.002, 'use_gpu': True, 'model': 'resnet18', 'num_clients': 2, 'num_global_rounds': 2, 'num_local_epochs': 1, 'print_debug_data': True, 'num_workers': 0, 'prefetch_factor': None, 'frames_image_mean': [0.337, 0.345, 0.367], 'frames_image_std': [0.16, 0.18, 0.214], 'dataset_root': '/mnt/ZOD', 'zenseact_dataset_root': '/staging/dataset_donation/round_2', 'checkpoint_path': None, 'start_from_checkpoint': False, 'use_ema': True, 'dataset_division': 'balanced', 'learning_rate': 0.001, 'target_distances': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85], 'loss': 'L1', 'gpu_id': 1}\n",
      "PyTorch=2.0.0+cu117. Pytorch vision=0.15.1+cu117. Flower=1.3.0\n",
      "Training will run on: cudas\n",
      "exp-3_federated_agent-0_resnet18_balanced_L1_85m_imgnet_normalized_1epochs_lr0.001_68.0trainImages_bs8_imgSize256_unfreezed_ema-True\n"
     ]
    }
   ],
   "source": [
    "# read unused frames\n",
    "with open(\"frames_with_less_than_165m_hp.json\") as f: UNUSED_FRAMES = set(json.load(f))\n",
    "\n",
    "# read the config\n",
    "with open(\"../config.json\") as f: configs = json.load(f)\n",
    "print(sys.argv)\n",
    "if(len(sys.argv) <= 1 or len(sys.argv[1]) > 2): \n",
    "    config = configs[-1]\n",
    "else:\n",
    "    config = [c for c in configs if c['exp_id'] == int(sys.argv[1])][0]\n",
    "print(config)\n",
    "\n",
    "# helper function to read from config\n",
    "c = lambda a : config[a]\n",
    "\n",
    "# specify the device\n",
    "DEVICE = torch.device(\"cuda\" if c('use_gpu') else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch={torch.__version__}. Pytorch vision={torchvision.__version__}. Flower={fl.__version__}\")\n",
    "print(f\"Training will run on: {DEVICE}s\")\n",
    "\n",
    "# path to tensor board persistent folders\n",
    "DISC = f\"exp-{c('exp_id')}_{c('type')}_agent-{c('agent_id')}_{c('model')}_{c('dataset_division')}_{c('loss')}_{c('target_distances')[-1]}m_imgnet_normalized_{c('num_local_epochs')}epochs_lr{c('learning_rate')}_{c('subset_factor')*34000}trainImages_bs{c('batch_size')}_imgSize{c('image_size')}_unfreezed_ema-{c('use_ema')}\"\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "TB_PATH = f\"TensorBoard/{DISC}_{now}\"\n",
    "TB_CENTRALIZED_SUB_PATH = \"TensorBoard_Centralized/loss/\"\n",
    "TB_FEDERATED_SUB_PATH = \"TensorBoard_Federated/loss/\"\n",
    "TB_SWARM_SUB_PATH = \"TensorBoard_Swarm/loss/\"\n",
    "print(DISC)\n",
    "\n",
    "# global tensorboard writer\n",
    "writer = SummaryWriter(TB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb08e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ground_truth(zod_frames, frame_id):\n",
    "    # get frame\n",
    "    zod_frame = zod_frames[frame_id]\n",
    "\n",
    "    # extract oxts\n",
    "    oxts = zod_frame.oxts\n",
    "\n",
    "    # get timestamp\n",
    "    key_timestamp = zod_frame.info.keyframe_time.timestamp()\n",
    "\n",
    "    try:\n",
    "        # get posses associated with frame timestamp\n",
    "        current_pose = oxts.get_poses(key_timestamp)\n",
    "\n",
    "        # transform poses\n",
    "        all_poses = oxts.poses[oxts.timestamps>=key_timestamp]\n",
    "        transformed_poses = np.linalg.pinv(current_pose) @ all_poses\n",
    "\n",
    "        # get translations\n",
    "        translations = transformed_poses[:, :3, 3]\n",
    "\n",
    "        # calculate acc diff distance\n",
    "        distances = np.linalg.norm(np.diff(translations, axis=0), axis=1)\n",
    "        accumulated_distances = np.cumsum(distances).astype(int).tolist()\n",
    "\n",
    "        # get the poses that each have a point having a distance from TARGET_DISTANCES\n",
    "        pose_idx = [accumulated_distances.index(i) for i in c('target_distances')]\n",
    "        used_poses = transformed_poses[pose_idx]\n",
    "\n",
    "    except:\n",
    "        print(\"detected invalid frame: \", frame_id)\n",
    "        return np.array([])\n",
    "\n",
    "    #print(used_poses.shape)\n",
    "    points = used_poses[:, :3, -1]\n",
    "    return points.flatten()\n",
    "   \n",
    "\n",
    "def save_dataset_tb_plot(tb_path, sample_distribution, subtitle, seed):\n",
    "    plt.bar(list(range(1, len(sample_distribution) + 1)), sample_distribution)\n",
    "    plt.xlabel(\"Partitions\")\n",
    "    plt.ylabel(\"Samples\")\n",
    "    plt.suptitle(\"Distribution of samples\")\n",
    "    plt.title(\"%s, seed: %s\" % (subtitle, seed)),\n",
    "\n",
    "    \"\"\"report to tensor board\"\"\"\n",
    "    writer.add_figure(\"sample_distribution/%s\" % (subtitle), plt.gcf(), global_step=0)\n",
    "\n",
    "\n",
    "def reshape_ground_truth(label, output_size=c('output_size')):\n",
    "    return label.reshape(((c('output_size') // 3), 3))\n",
    "\n",
    "def visualize_HP_on_image(zod_frames, frame_id, preds=None, showImg=True):\n",
    "    \"\"\"Visualize oxts track on image plane.\"\"\"\n",
    "    camera=Camera.FRONT\n",
    "    zod_frame = zod_frames[frame_id]\n",
    "    image = zod_frame.get_image(Anonymization.DNAT)\n",
    "    calibs = zod_frame.calibration\n",
    "    points_gt = get_ground_truth(zod_frames, frame_id)\n",
    "    preds_row = preds.copy()\n",
    "    points = reshape_ground_truth(points_gt)\n",
    "    \n",
    "    circle_size = 15\n",
    "    \n",
    "    # transform point to camera coordinate system\n",
    "    T_inv = np.linalg.pinv(calibs.get_extrinsics(camera).transform)\n",
    "    camerapoints = transform_points(points[:, :3], T_inv)\n",
    "\n",
    "    # filter points that are not in the camera field of view\n",
    "    points_in_fov = get_points_in_camera_fov(calibs.cameras[camera].field_of_view, camerapoints)\n",
    "    points_in_fov = points_in_fov[0]\n",
    "\n",
    "    # project points to image plane\n",
    "    xy_array = project_3d_to_2d_kannala(\n",
    "        points_in_fov,\n",
    "        calibs.cameras[camera].intrinsics[..., :3],\n",
    "        calibs.cameras[camera].distortion,\n",
    "    )\n",
    "    \n",
    "    ground_truth_color = (19, 80, 41)\n",
    "    preds_color = (161, 65, 137)\n",
    "    \n",
    "    points = []\n",
    "    for i in range(xy_array.shape[0]):\n",
    "        x, y = int(xy_array[i, 0]), int(xy_array[i, 1])\n",
    "        cv2.circle(image, (x,y), circle_size, ground_truth_color, -1)\n",
    "        points.append([x,y])\n",
    "    \n",
    "    \"\"\"Draw a line in image.\"\"\"\n",
    "    def draw_line(image, line, color):\n",
    "        return cv2.polylines(image.copy(), [np.round(line).astype(np.int32)], isClosed=False, color=color, thickness=20)\n",
    "    \n",
    "    image = draw_line(image, points, ground_truth_color)\n",
    "    \n",
    "    # transform and draw predictions \n",
    "    if(preds is not None):\n",
    "        preds = reshape_ground_truth(preds)\n",
    "        predpoints = transform_points(preds[:, :3], T_inv)\n",
    "        predpoints_in_fov = get_points_in_camera_fov(calibs.cameras[camera].field_of_view, predpoints)\n",
    "        predpoints_in_fov = predpoints_in_fov[0]\n",
    "        \n",
    "        xy_array_preds = project_3d_to_2d_kannala(\n",
    "            predpoints_in_fov,\n",
    "            calibs.cameras[camera].intrinsics[..., :3],\n",
    "            calibs.cameras[camera].distortion,\n",
    "        )\n",
    "        preds = []\n",
    "        for i in range(xy_array_preds.shape[0]):\n",
    "            x, y = int(xy_array_preds[i, 0]), int(xy_array_preds[i, 1])\n",
    "            cv2.circle(image, (x,y), circle_size, preds_color, -1)\n",
    "            preds.append([x,y])\n",
    "        \n",
    "        #preds = preds[:(len(preds)//2)]\n",
    "        image = draw_line(image, preds, preds_color)\n",
    "        \n",
    "    #plt.imsave(f'inference_{frame_id}.png', image)\n",
    "    if(showImg):\n",
    "        plt.clf()\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image)\n",
    "    return image, points_gt, preds_row\n",
    "    \n",
    "def visualize_multiple(zod_frames, frame_ids, model=None):\n",
    "    if(model):\n",
    "        images = [visualize_HP_on_image(zod_frames, frame_id, predict(model.to(DEVICE), zod_frames, frame_id), showImg=False) for frame_id in frame_ids]\n",
    "    else:\n",
    "        images = [visualize_HP_on_image(zod_frames, frame_id, None, showImg=False) for frame_id in frame_ids]\n",
    "        \n",
    "    plt.figure(figsize=(60,60))\n",
    "    columns = 4\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(len(images) // columns + 1, columns, i + 1)\n",
    "        plt.gca().set_title(frame_ids[i], fontsize=20)\n",
    "        plt.imshow(image[0])  \n",
    "    \n",
    "def get_transformed_image(zod_frames, frame_id):\n",
    "    frame = zod_frames[frame_id]\n",
    "    image_path = frame.info.get_key_camera_frame(Anonymization.DNAT).filepath\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    image = np.array(Image.fromarray(image).resize((c('image_size'), c('image_size')), Image.BILINEAR))\n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=(c('image_size'), c('image_size')), antialias=True)\n",
    "        ])\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    return image\n",
    "\n",
    "def predict(model, zod_frames, frame_id):\n",
    "    image = get_transformed_image(zod_frames, frame_id).to(DEVICE)\n",
    "    outputs = model(image).squeeze(0)\n",
    "    label = torch.from_numpy(get_ground_truth(zod_frames, frame_id)).to(DEVICE)\n",
    "    \n",
    "    loss = torch.nn.L1Loss(label, outputs)\n",
    "\n",
    "    print(f'frame: {frame_id}, loss: {loss.item()}')\n",
    "    preds = outputs.cpu().detach().numpy()\n",
    "    return preds\n",
    "\n",
    "def load_HP(dataset_root):\n",
    "    zod_frames = ZodFrames(dataset_root=dataset_root, version='full')\n",
    "    training_frames_all = zod_frames.get_split(constants.TRAIN)\n",
    "    validation_frames_all = zod_frames.get_split(constants.VAL)\n",
    "\n",
    "    return zod_frames, training_frames_all, validation_frames_all\n",
    "\n",
    "def is_valid(frame_id):\n",
    "    return frame_id not in UNUSED_FRAMES\n",
    "\n",
    "def save_to_json(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(json.dumps(data))\n",
    "        \n",
    "def load_from_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d5ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZODImporter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=c('dataset_root'),\n",
    "        subset_factor=c('subset_factor'),\n",
    "        img_size=c('image_size'),\n",
    "        batch_size=c('batch_size'),\n",
    "        tb_path=TB_PATH,\n",
    "        zod_frames=None,\n",
    "        training_frames=None, \n",
    "        validation_frames=None\n",
    "    ):\n",
    "        if(zod_frames == None):\n",
    "            self.zod_frames = ZodFrames(dataset_root=root, version='full')\n",
    "\n",
    "            self.training_frames_all = self.zod_frames.get_split(constants.TRAIN)\n",
    "            self.validation_frames_all = self.zod_frames.get_split(constants.VAL)\n",
    "            \n",
    "            self.training_frames, self.validation_frames = self.get_train_val_ids(\n",
    "                self.training_frames_all, \n",
    "                self.validation_frames_all, \n",
    "                subset_factor)\n",
    "        else:\n",
    "            self.zod_frames = zod_frames\n",
    "            self.training_frames = training_frames\n",
    "            self.validation_frames = validation_frames\n",
    "            \n",
    "        print(\"length of training_frames subset:\", len(self.training_frames))\n",
    "        print(\"length of validation_frames subset:\", len(self.validation_frames))\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tb_path = tb_path\n",
    "    \n",
    "    def get_train_val_ids(self, training_frames_all, validation_frames_all, subset_factor):\n",
    "        if(c('dataset_division') == 'balanced'):\n",
    "            with open(\"../balanced_train_ids.txt\") as f:\n",
    "                training_frames_all = json.load(f)\n",
    "                print(f'balanced sample: {training_frames_all[:5]}')\n",
    "\n",
    "        training_frames = list(training_frames_all)[: int(len(training_frames_all) * subset_factor)]\n",
    "        validation_frames = list(validation_frames_all)[: int(len(validation_frames_all) * subset_factor)]\n",
    "\n",
    "        training_frames = [x for x in tqdm(training_frames) if is_valid(x)]\n",
    "        validation_frames = [x for x in tqdm(validation_frames) if is_valid(x)]\n",
    "\n",
    "        return training_frames, validation_frames\n",
    "        \n",
    "    def is_valid(self, frame_id):\n",
    "        return frame_id not in UNUSED_FRAMES\n",
    "        \n",
    "    def load_datasets(self, num_clients=c('num_clients')):\n",
    "        seed = 42\n",
    "        imagenet_mean=[0.485, 0.456, 0.406]\n",
    "        imagenet_std=[0.229, 0.224, 0.225]\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "            transforms.Resize(size=(self.img_size, self.img_size), antialias=True)\n",
    "        ])\n",
    "\n",
    "        trainset = ZodDataset(zod_frames=self.zod_frames, frames_id_set=self.training_frames, transform=transform)\n",
    "        testset = ZodDataset(zod_frames=self.zod_frames, frames_id_set=self.validation_frames, transform=transform)\n",
    "\n",
    "        # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "        partition_size = len(trainset) // num_clients\n",
    "\n",
    "        lengths = [partition_size]\n",
    "        if num_clients > 1:\n",
    "            lengths = [partition_size] * (num_clients - 1)\n",
    "            lengths.append(len(trainset) - sum(lengths))\n",
    "\n",
    "        datasets = random_split(trainset, lengths, torch.Generator().manual_seed(seed))\n",
    "\n",
    "        # Split each partition into train/val and create DataLoader\n",
    "        trainloaders, valloaders = [], []\n",
    "        lengths_train, lengths_val = [], []\n",
    "        for ds in datasets:\n",
    "            len_val = int(len(ds) * c('val_factor'))\n",
    "            len_train = int(len(ds) - len_val)\n",
    "            lengths_train.append(len_train)\n",
    "            lengths_val.append(len_val)\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(seed))\n",
    "            trainloaders.append(DataLoader(ds_train,batch_size=self.batch_size, shuffle=True, num_workers=c('num_workers')))\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=self.batch_size, num_workers=c('num_workers')))\n",
    "\n",
    "        len_complete_val = int(len(trainset) * c('val_factor'))\n",
    "        len_complete_train = int(len(trainset) - len_complete_val)\n",
    "        train_split, val_split = random_split(\n",
    "            trainset,\n",
    "            [len_complete_train, len_complete_val],\n",
    "            torch.Generator().manual_seed(seed),\n",
    "        )\n",
    "\n",
    "        completeTrainloader = DataLoader(\n",
    "            train_split, batch_size=self.batch_size, num_workers=c('num_workers'), shuffle=True, \n",
    "            prefetch_factor=c('prefetch_factor'),\n",
    "            pin_memory= True)\n",
    "        \n",
    "        completeValloader = DataLoader(\n",
    "            val_split, batch_size=self.batch_size, num_workers=c('num_workers'), shuffle=True,\n",
    "            prefetch_factor=c('prefetch_factor'),\n",
    "            pin_memory= True)\n",
    "\n",
    "        testloader = DataLoader(testset, batch_size=self.batch_size, num_workers=c('num_workers'))\n",
    "\n",
    "        \"\"\"report to tensor board\"\"\"\n",
    "        save_dataset_tb_plot(self.tb_path, lengths_train, \"training\", seed)\n",
    "        save_dataset_tb_plot(self.tb_path, lengths_val, \"validation\", seed)\n",
    "        save_dataset_tb_plot(self.tb_path, [len(testset)], \"testing\", seed)\n",
    "\n",
    "        return (\n",
    "            trainloaders,\n",
    "            valloaders,\n",
    "            testloader,\n",
    "            completeTrainloader,\n",
    "            completeValloader,\n",
    "        )\n",
    "\n",
    "class ZodDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        zod_frames,\n",
    "        frames_id_set,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "        self.zod_frames = zod_frames\n",
    "        self.frames_id_set = frames_id_set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames_id_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # load frame\n",
    "        frame_idx = self.frames_id_set[idx]\n",
    "        frame = self.zod_frames[frame_idx]\n",
    "        \n",
    "        # get image\n",
    "        image_path = frame.info.get_key_camera_frame(Anonymization.DNAT).filepath\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        \n",
    "        # extract ground truth\n",
    "        label = get_ground_truth(self.zod_frames, frame_idx)\n",
    "        \n",
    "        # create sample\n",
    "        sample = dict(image=image, label=label)\n",
    "        \n",
    "        # resize images\n",
    "        image = np.array(Image.fromarray(sample[\"image\"]).resize((c('image_size'), c('image_size')), Image.BILINEAR))\n",
    "        \n",
    "        # convert to other format HWC -> CHW\n",
    "        #sample[\"image\"] = np.moveaxis(image, -1, 0)\n",
    "        sample[\"label\"] = np.expand_dims(label, 0).astype(np.float32)\n",
    "        \n",
    "        if(self.transform):\n",
    "            sample[\"image\"] = self.transform(sample[\"image\"])\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bc97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT_Model(pl.LightningModule):\n",
    "    def __init__(self, cid=0) -> None:\n",
    "        super(PT_Model, self).__init__()\n",
    "        \n",
    "        self.model = None\n",
    "        if(c('model') == 'resnet18'):\n",
    "            self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        elif(c('model') == 'mobile_net'):\n",
    "            self.model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.change_head_net()\n",
    "        self.useEma = c('use_ema')\n",
    "        self.ema = None\n",
    "\n",
    "        if(self.useEma):\n",
    "            self.ema = EMA(\n",
    "                self.model,\n",
    "                beta=0.995,\n",
    "                update_after_step=100,\n",
    "                power=3/4,\n",
    "                inv_gamma=1.0\n",
    "            )\n",
    "\n",
    "        self.is_pretrained = True\n",
    "        self.loss_fn = torch.nn.L1Loss()\n",
    "        self.cid = cid\n",
    "        \n",
    "        # pytorch imagenet calculated mean/std\n",
    "        self.mean=[0.485, 0.456, 0.406]\n",
    "        self.std=[0.229, 0.224, 0.225]\n",
    "        \n",
    "        self.epoch_counter = 1\n",
    "\n",
    "        self.inter_train_outputs = []\n",
    "        self.inter_train_ema_outputs = []\n",
    "\n",
    "        self.inter_val_outputs = []\n",
    "        self.inter_val_ema_outputs = []\n",
    "\n",
    "        self.inter_test_outputs = []\n",
    "        self.inter_test_ema_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        label = self.model(image)\n",
    "\n",
    "        if(self.useEma):\n",
    "            ema_label = self.ema(image)\n",
    "            return label, ema_label\n",
    "\n",
    "        return label, None\n",
    "\n",
    "    def model_parameters(self):\n",
    "        return self.model.fc.parameters()\n",
    "\n",
    "    def change_head_net(self):\n",
    "        num_ftrs = 0\n",
    "\n",
    "        if(c('model') == 'resnet18'):\n",
    "            num_ftrs = self.model.fc.in_features\n",
    "        elif(c('model') == 'mobile_net'):\n",
    "            num_ftrs = self.model.classifier[-1].in_features\n",
    "\n",
    "        head_net = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1024, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, c('output_size'), bias=True),\n",
    "        )\n",
    "\n",
    "        if(c('model') == 'resnet18'):\n",
    "            self.model.fc = head_net\n",
    "        elif(c('model') == 'mobile_net'):\n",
    "            self.model.classifier[-1] = head_net\n",
    "            \n",
    "    def shared_step(self, batch, batch_idx, stage, inter_outputs, ema_inter_outputs):\n",
    "        image = batch[\"image\"]\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        logits_label, ema_logits_label = self.forward(image)\n",
    "        logits_label = logits_label.unsqueeze(dim=1)\n",
    "        loss = self.loss_fn(logits_label, label)\n",
    "\n",
    "        if(self.useEma):\n",
    "            ema_logits_label = ema_logits_label.unsqueeze(dim=1)\n",
    "            ema_loss = self.loss_fn(ema_logits_label, label)\n",
    "\n",
    "        ema_loss = ema_loss if(self.useEma) else None\n",
    "        \n",
    "        inter_outputs.append(loss.item())\n",
    "        if(self.useEma):\n",
    "            ema_inter_outputs.append(ema_loss.item())\n",
    "            if(stage == 'train'):\n",
    "                self.ema.update()\n",
    "\n",
    "        if(batch_idx == 1 and stage != 'test'):\n",
    "            self.updateTB(inter_outputs, stage)\n",
    "            \n",
    "            if(self.useEma):\n",
    "                self.updateTB(ema_inter_outputs, f'{stage}_ema')\n",
    "\n",
    "            if(stage == 'valid'):\n",
    "                self.epoch_counter +=1\n",
    "\n",
    "            print('DEBUG self.inter_train_outputs', self.inter_train_outputs)\n",
    "            print('DEBUG Epoch loss:', loss.item())\n",
    "            print('DEBUG Ema epoch loss:', ema_loss.item())\n",
    "        \n",
    "        print('DEBUG batch loss:', loss.item())\n",
    "        print('DEBUG Ema batch loss:', ema_loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def shared_epoch_end(self, inter_outputs, ema_inter_outputs, stage):\n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": inter_outputs[-1],\n",
    "            f\"{stage}_ema_loss\": ema_inter_outputs[-1] if(self.useEma) else None,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx, 'train', self.inter_train_outputs, self.inter_train_ema_outputs)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.inter_train_outputs, self.inter_train_ema_outputs, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx, 'valid', self.inter_val_outputs, self.inter_val_ema_outputs)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.inter_val_outputs, self.inter_val_ema_outputs, 'valid')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx, 'test', self.inter_test_outputs, self.inter_test_ema_outputs)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.inter_test_outputs, self.inter_test_ema_outputs, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=c('learning_rate'))\n",
    "    \n",
    "    def compute_metrics(pred_trajectory, target_trajectory):\n",
    "        # L1 and L2 distance: matrix of size BSx40x3\n",
    "        L1_loss = torch.abs(pred_trajectory - target_trajectory)\n",
    "        L2_loss = torch.pow(pred_trajectory - target_trajectory, 2)\n",
    "\n",
    "        # BSx40x3 -> BSx3 average over the predicted points\n",
    "        L1_loss = L1_loss.mean(axis=1)\n",
    "        L2_loss = L2_loss.mean(axis=1)\n",
    "\n",
    "        # split into losses for each axis and an avg loss across 3 axes\n",
    "        # All returned tensors have shape (BS)\n",
    "        return {\n",
    "                'L1_loss':   L1_loss.mean(axis=1),\n",
    "                'L1_loss_x': L1_loss[:, 0],\n",
    "                'L1_loss_y': L1_loss[:, 1],\n",
    "                'L1_loss_z': L1_loss[:, 2],\n",
    "                'L2_loss':   L2_loss.mean(axis=1),\n",
    "                'L2_loss_x': L2_loss[:, 0],\n",
    "                'L2_loss_y': L2_loss[:, 1],\n",
    "                'L2_loss_z': L2_loss[:, 2]}\n",
    "    \n",
    "    def get_TB_path(self):\n",
    "        if(c('type')=='centralized'):\n",
    "            return TB_CENTRALIZED_SUB_PATH\n",
    "        if(c('type')=='federated'):\n",
    "            return f\"{TB_FEDERATED_SUB_PATH}{self.cid}/\",\n",
    "\n",
    "    def updateTB(self, inter_outputs, stage):\n",
    "        epoch_loss = np.mean(inter_outputs) \n",
    "        writer.add_scalars(self.get_TB_path(), {stage: epoch_loss},self.epoch_counter)\n",
    "        inter_outputs = [epoch_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4509d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, nr_epochs=c('num_local_epochs')):\n",
    "    trainer = get_trainer()\n",
    "\n",
    "    trainer.fit(\n",
    "        model, \n",
    "        train_dataloaders=train_dataloader, \n",
    "        val_dataloaders=valid_dataloader,\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def validate(model, valid_dataloader):\n",
    "    trainer = get_trainer()\n",
    "    valid_metrics = trainer.validate(model, dataloaders=valid_dataloader, verbose=False)\n",
    "    pprint(valid_metrics)\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    trainer = get_trainer()\n",
    "    test_metrics = trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    pprint(test_metrics)\n",
    "    return test_metrics\n",
    "\n",
    "def get_trainer():\n",
    "    return pl.Trainer(\n",
    "        accelerator= 'gpu',\n",
    "        max_epochs=c('num_local_epochs'),\n",
    "        devices=[c('gpu_id')],\n",
    "    )\n",
    "\n",
    "def net_instance(name):\n",
    "    print(f\"🌻 Created new model - {name} 🌻\")\n",
    "    return PT_Model()\n",
    "\n",
    "def get_parameters(net, cid):\n",
    "    print(f\"⤺ Get model parameters of client {cid}\")\n",
    "    return [val.cpu().numpy() for _, val in net.model.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray], cid):\n",
    "    print(f\"⤻ Set model parameters of client {cid}\")\n",
    "    params_dict = zip(net.model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict(\n",
    "        {\n",
    "            k: torch.Tensor(v) if v.shape != torch.Size([]) else torch.Tensor([0])\n",
    "            for k, v in params_dict\n",
    "        }\n",
    "    )\n",
    "    net.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def save_model(net, name):\n",
    "    print(f\"🔒 Saved the model of client {name} to the disk. 🔒\")\n",
    "    torch.save(net.model.state_dict(), f\"{name}.pth\")\n",
    "\n",
    "def load_model(name):\n",
    "    print(f\"🛅 Loaded the model of client {name} from the disk. 🛅\")\n",
    "    net = net_instance(f\"{name}\")\n",
    "    net.model.load_state_dict(torch.load(f\"{name}.pth\"))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd575a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08b1832693d4fd7945ca7c2258e3347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading infos: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced sample: ['003298', '074927', '038839', '021686', '088515']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 199457.39it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 7936.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training_frames subset: 70\n",
      "length of validation_frames subset: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get loaders\n",
    "trainloaders, valloaders, testloader, completeTrainloader, completeValloader = ZODImporter().load_datasets(num_clients=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "581c9ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | model   | ResNet | 12.3 M\n",
      "1 | ema     | EMA    | 24.5 M\n",
      "2 | loss_fn | L1Loss | 0     \n",
      "-----------------------------------\n",
      "12.3 M    Trainable params\n",
      "12.3 M    Non-trainable params\n",
      "24.5 M    Total params\n",
      "98.022    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ec2f968bd04819b9a8882b0e6baeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasan/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/yasan/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG batch loss: 16.392702102661133\n",
      "DEBUG Ema batch loss: 16.392702102661133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasan/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/yasan/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c547fa1ac7694b07b87b39659b72e07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG batch loss: 16.660438537597656\n",
      "DEBUG Ema batch loss: 16.660438537597656\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m PT_Model\u001b[39m.\u001b[39mload_from_checkpoint(c(\u001b[39m'\u001b[39m\u001b[39mcheckpoint_path\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mif\u001b[39;00m(c(\u001b[39m'\u001b[39m\u001b[39mstart_from_checkpoint\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39melse\u001b[39;00m PT_Model()\n\u001b[1;32m      4\u001b[0m \u001b[39m# train supervised\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train(model, completeTrainloader, completeValloader, nr_epochs\u001b[39m=\u001b[39;49mc(\u001b[39m'\u001b[39;49m\u001b[39mnum_local_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      7\u001b[0m \u001b[39m# validate \u001b[39;00m\n\u001b[1;32m      8\u001b[0m validate(model, completeValloader)\n",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, valid_dataloader, nr_epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, train_dataloader, valid_dataloader, nr_epochs\u001b[39m=\u001b[39mc(\u001b[39m'\u001b[39m\u001b[39mnum_local_epochs\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m      2\u001b[0m     trainer \u001b[39m=\u001b[39m get_trainer()\n\u001b[0;32m----> 4\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      5\u001b[0m         model, \n\u001b[1;32m      6\u001b[0m         train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader, \n\u001b[1;32m      7\u001b[0m         val_dataloaders\u001b[39m=\u001b[39;49mvalid_dataloader,\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    217\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[1;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         closure()\n\u001b[1;32m    180\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    187\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:261\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    260\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    262\u001b[0m     trainer,\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    264\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    265\u001b[0m     batch_idx,\n\u001b[1;32m    266\u001b[0m     optimizer,\n\u001b[1;32m    267\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    268\u001b[0m )\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/core/module.py:1265\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1232\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1233\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1265\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:158\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:224\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 224\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/torch/optim/adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 121\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    124\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     94\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     95\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:308\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    307\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    311\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:366\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    365\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m, in \u001b[0;36mPT_Model.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 123\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_step(batch, batch_idx, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minter_train_outputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minter_train_ema_outputs)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[7], line 97\u001b[0m, in \u001b[0;36mPT_Model.shared_step\u001b[0;34m(self, batch, batch_idx, stage, inter_outputs, ema_inter_outputs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mema\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m(batch_idx \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m stage \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdateTB(inter_outputs, stage)\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39museEma):\n\u001b[1;32m    100\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateTB(ema_inter_outputs, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m}\u001b[39;00m\u001b[39m_ema\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 175\u001b[0m, in \u001b[0;36mPT_Model.updateTB\u001b[0;34m(self, inter_outputs, stage)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdateTB\u001b[39m(\u001b[39mself\u001b[39m, inter_outputs, stage):\n\u001b[1;32m    174\u001b[0m     epoch_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(inter_outputs) \n\u001b[0;32m--> 175\u001b[0m     writer\u001b[39m.\u001b[39;49madd_scalars(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_TB_path(), {stage: epoch_loss},\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_counter)\n\u001b[1;32m    176\u001b[0m     inter_outputs \u001b[39m=\u001b[39m [epoch_loss]\n",
      "File \u001b[0;32m~/miniconda3/envs/zen/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:426\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalars\u001b[0;34m(self, main_tag, tag_scalar_dict, global_step, walltime)\u001b[0m\n\u001b[1;32m    424\u001b[0m fw_logdir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39mget_logdir()\n\u001b[1;32m    425\u001b[0m \u001b[39mfor\u001b[39;00m tag, scalar_value \u001b[39min\u001b[39;00m tag_scalar_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 426\u001b[0m     fw_tag \u001b[39m=\u001b[39m fw_logdir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m main_tag\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m tag\n\u001b[1;32m    427\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[39mif\u001b[39;00m fw_tag \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = PT_Model.load_from_checkpoint(c('checkpoint_path')) if(c('start_from_checkpoint')) else PT_Model()\n",
    "\n",
    "# train supervised\n",
    "train(model, completeTrainloader, completeValloader, nr_epochs=c('num_local_epochs'))\n",
    "\n",
    "# validate \n",
    "validate(model, completeValloader)\n",
    "\n",
    "# test\n",
    "test(model, testloader)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
