{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchsummary import summary\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from numba import cuda\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import enum\n",
    "import threading\n",
    "from matplotlib import pyplot as plt\n",
    "from zod import ZodFrames\n",
    "from zod import ZodSequences\n",
    "import zod.constants as constants\n",
    "from zod.constants import Camera, Lidar, Anonymization, AnnotationProject\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import cv2\n",
    "from flask import Flask, request, jsonify\n",
    "import multiprocessing\n",
    "from zod.visualization.oxts_on_image import visualize_oxts_on_image\n",
    "from zod.constants import Camera\n",
    "from zod.data_classes.calibration import Calibration\n",
    "from zod.data_classes.oxts import EgoMotion\n",
    "from zod.utils.polygon_transformations import polygons_to_binary_mask\n",
    "from zod.utils.geometry import (\n",
    "    get_points_in_camera_fov,\n",
    "    project_3d_to_2d_kannala,\n",
    "    transform_points,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7763a",
   "metadata": {},
   "source": [
    "## static params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTPUT = 51\n",
    "IMG_SIZE = 256\n",
    "RUN_PRETRAINED = True\n",
    "BATCH_SIZE = 8\n",
    "VAL_FACTOR = 0.15\n",
    "SUBSET_FACTOR = 0.03\n",
    "USE_GPU = True\n",
    "NUM_GLOBAL_ROUNDS = 3\n",
    "NUM_LOCAL_EPOCHS = 3\n",
    "PRINT_DEBUG_DATA = True\n",
    "NUM_WORKERS = 4 # os.cpu_count()\n",
    "FRAMES_IMAGE_MEAN = [0.337, 0.345, 0.367]\n",
    "FRAMES_IMAGE_STD = [0.160, 0.180, 0.214]\n",
    "DEVICE = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "TRAIN_FRAMES_PATH = \"../GroundTruth/training_seg_annotated_frames.json\"\n",
    "VAL_FRAMES_PATH = \"../GroundTruth/validation_seg_annotated_frames.json\"\n",
    "STORED_GROUND_TRUTH_PATH = \"cached_gt/hp_gt_smp.json\"\n",
    "STORED_BALANCED_DS_PATH = \"cached_gt/balanced_frames.txt\"\n",
    "DATASET_ROOT = \"/mnt/ZOD\"\n",
    "ZENSEACT_DATASET_ROOT = \"/staging/dataset_donation/round_2\"\n",
    "\n",
    "TARGET_DISTANCES = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 95, 110, 125, 145, 165]\n",
    "\n",
    "with open(\"frames_with_less_than_165m_hp.json\") as f:\n",
    "    short_frames = json.load(f)\n",
    "UNUSED_FRAMES = set(short_frames)\n",
    "\n",
    "print(f\"PyTorch={torch.__version__}. Pytorch vision={torchvision.__version__}. Flower={fl.__version__}\")\n",
    "print(f\"Training will run on: {DEVICE}s\")\n",
    "\n",
    "\"\"\" path to tensor board persistent folders\"\"\"\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "TB_PATH = f\"TensorBoard/runs{now}\"\n",
    "TB_CENTRALIZED_SUB_PATH = \"TensorBoard_Centralized/loss\"\n",
    "TB_FEDERATED_SUB_PATH = \"TensorBoard_Federated/loss\"\n",
    "TB_SWARM_SUB_PATH = \"TensorBoard_Swarm/loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4eaf03",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZODImporter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=DATASET_ROOT,\n",
    "        subset_factor=SUBSET_FACTOR,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        tb_path=TB_PATH,\n",
    "        zod_frames=None,\n",
    "        training_frames=None, \n",
    "        validation_frames=None\n",
    "    ):\n",
    "        if(zod_frames == None):\n",
    "            self.zod_frames = ZodFrames(dataset_root=root, version='full')\n",
    "            self.training_frames_all = self.zod_frames.get_split(constants.TRAIN)\n",
    "            self.validation_frames_all = self.zod_frames.get_split(constants.VAL)\n",
    "            \n",
    "            self.training_frames = list(self.training_frames_all)[: int(len(self.training_frames_all) * subset_factor)]\n",
    "            self.validation_frames = list(self.validation_frames_all)[: int(len(self.validation_frames_all) * subset_factor)]\n",
    "\n",
    "            self.training_frames = [x for x in tqdm(self.training_frames) if self.is_valid(x)]\n",
    "            self.validation_frames = [x for x in tqdm(self.validation_frames) if self.is_valid(x)]\n",
    "        else:\n",
    "            self.zod_frames = zod_frames\n",
    "            self.training_frames = training_frames\n",
    "            self.validation_frames = validation_frames\n",
    "            \n",
    "        print(\"length of training_frames subset:\", len(self.training_frames))\n",
    "        print(\"length of test_frames subset:\", len(self.validation_frames))\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tb_path = tb_path\n",
    "\n",
    "        \n",
    "    def is_valid(self, frame_id):\n",
    "        return frame_id not in UNUSED_FRAMES\n",
    "        \n",
    "    def load_datasets(self, num_clients: int):\n",
    "        seed = 42\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(FRAMES_IMAGE_MEAN, FRAMES_IMAGE_STD),\n",
    "            transforms.Resize(size=(self.img_size, self.img_size), antialias=True)\n",
    "        ])\n",
    "\n",
    "        trainset = ZodDataset(zod_frames=self.zod_frames, frames_id_set=self.training_frames, transform=transform)\n",
    "        testset = ZodDataset(zod_frames=self.zod_frames, frames_id_set=self.validation_frames, transform=transform)\n",
    "\n",
    "        # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "        partition_size = len(trainset) // num_clients\n",
    "\n",
    "        lengths = [partition_size]\n",
    "        if num_clients > 1:\n",
    "            lengths = [partition_size] * (num_clients - 1)\n",
    "            lengths.append(len(trainset) - sum(lengths))\n",
    "\n",
    "        datasets = random_split(trainset, lengths, torch.Generator().manual_seed(seed))\n",
    "\n",
    "        # Split each partition into train/val and create DataLoader\n",
    "        trainloaders, valloaders = [], []\n",
    "        lengths_train, lengths_val = [], []\n",
    "        for ds in datasets:\n",
    "            len_val = int(len(ds) * VAL_FACTOR)\n",
    "            len_train = int(len(ds) - len_val)\n",
    "            lengths_train.append(len_train)\n",
    "            lengths_val.append(len_val)\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(seed))\n",
    "            trainloaders.append(DataLoader(ds_train,batch_size=self.batch_size, shuffle=True, num_workers=NUM_WORKERS))\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=self.batch_size, num_workers=NUM_WORKERS))\n",
    "\n",
    "        len_complete_val = int(len(trainset) * VAL_FACTOR)\n",
    "        len_complete_train = int(len(trainset) - len_complete_val)\n",
    "        train_split, val_split = random_split(\n",
    "            trainset,\n",
    "            [len_complete_train, len_complete_val],\n",
    "            torch.Generator().manual_seed(seed),\n",
    "        )\n",
    "\n",
    "        completeTrainloader = DataLoader(train_split, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n",
    "        completeValloader = DataLoader(val_split, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "        testloader = DataLoader(testset, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "        \"\"\"report to tensor board\"\"\"\n",
    "        save_dataset_tb_plot(self.tb_path, lengths_train, \"training\", seed)\n",
    "        save_dataset_tb_plot(self.tb_path, lengths_val, \"validation\", seed)\n",
    "\n",
    "        return (\n",
    "            trainloaders,\n",
    "            valloaders,\n",
    "            testloader,\n",
    "            completeTrainloader,\n",
    "            completeValloader,\n",
    "        )\n",
    "\n",
    "class ZodDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        zod_frames,\n",
    "        frames_id_set,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "        self.zod_frames = zod_frames\n",
    "        self.frames_id_set = frames_id_set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames_id_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # load frame\n",
    "        frame_idx = self.frames_id_set[idx]\n",
    "        frame = self.zod_frames[frame_idx]\n",
    "        \n",
    "        # get image\n",
    "        image_path = frame.info.get_key_camera_frame(Anonymization.DNAT).filepath\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        # extract ground truth\n",
    "        label = self.get_ground_truth(frame_idx)\n",
    "        \n",
    "        # create sample\n",
    "        sample = dict(image=image, label=label)\n",
    "        \n",
    "        # resize images\n",
    "        image = np.array(Image.fromarray(sample[\"image\"]).resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR))\n",
    "\n",
    "        # convert to other format HWC -> CHW\n",
    "        sample[\"image\"] = np.moveaxis(image, -1, 0)\n",
    "        sample[\"label\"] = np.expand_dims(label, 0)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    def get_ground_truth(self, frame_id):\n",
    "        # get frame\n",
    "        zod_frame = self.zod_frames[frame_id]\n",
    "\n",
    "        # extract oxts\n",
    "        oxts = zod_frame.oxts\n",
    "\n",
    "        # get timestamp\n",
    "        key_timestamp = zod_frame.info.keyframe_time.timestamp()\n",
    "\n",
    "        try:\n",
    "            # get posses associated with frame timestamp\n",
    "            current_pose = oxts.get_poses(key_timestamp)\n",
    "\n",
    "            # transform poses\n",
    "            all_poses = oxts.poses\n",
    "            transformed_poses = np.linalg.pinv(current_pose) @ all_poses\n",
    "\n",
    "            # get translations\n",
    "            translations = transformed_poses[:, :3, 3]\n",
    "\n",
    "            # calculate acc diff distance\n",
    "            distances = np.linalg.norm(np.diff(translations, axis=0), axis=1)\n",
    "            accumulated_distances = np.cumsum(distances).astype(int).tolist()\n",
    "\n",
    "            # get the poses that each have a point having a distance from TARGET_DISTANCES\n",
    "            pose_idx = [accumulated_distances.index(i) for i in TARGET_DISTANCES]\n",
    "            used_poses = transformed_poses[pose_idx]\n",
    "\n",
    "        except:\n",
    "            #print(\"detected invalid frame: \", frame_id)\n",
    "            return np.array([])\n",
    "\n",
    "        #print(used_poses.shape)\n",
    "        points = used_poses[:, :3, -1]\n",
    "        return points.flatten()\n",
    "\n",
    "\n",
    "\n",
    "def save_dataset_tb_plot(tb_path, sample_distribution, subtitle, seed):\n",
    "    plt.bar(list(range(1, len(sample_distribution) + 1)), sample_distribution)\n",
    "    plt.xlabel(\"Partitions\")\n",
    "    plt.ylabel(\"Samples\")\n",
    "    plt.suptitle(\"Distribution of samples\")\n",
    "    plt.title(\"%s, seed: %s\" % (subtitle, seed)),\n",
    "\n",
    "    \"\"\"report to tensor board\"\"\"\n",
    "    writer = SummaryWriter(tb_path)\n",
    "    writer.add_figure(\"sample_distribution/%s\" % (subtitle), plt.gcf(), global_step=0)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def reshape_ground_truth(label, output_size=NUM_OUTPUT):\n",
    "    return label.reshape(((NUM_OUTPUT // 3), 3))\n",
    "\n",
    "def visualize_HP_on_image(zod_frames, frame_id, preds=None):\n",
    "    \"\"\"Visualize oxts track on image plane.\"\"\"\n",
    "    camera = Camera.FRONT\n",
    "    zod_frame = zod_frames[frame_id]\n",
    "    image = zod_frame.get_image(Anonymization.DNAT)\n",
    "    calibs = zod_frame.calibration\n",
    "    points = get_ground_truth(zod_frames, frame_id)\n",
    "    points = reshape_ground_truth(points)\n",
    "\n",
    "    # transform point to camera coordinate system\n",
    "    T_inv = np.linalg.pinv(calibs.get_extrinsics(camera).transform)\n",
    "    camerapoints = transform_points(points[:, :3], T_inv)\n",
    "    print(f\"Number of points: {points.shape[0]}\")\n",
    "\n",
    "    # filter points that are not in the camera field of view\n",
    "    points_in_fov = get_points_in_camera_fov(\n",
    "        calibs.cameras[camera].field_of_view, camerapoints\n",
    "    )\n",
    "    print(f\"Number of points in fov: {len(points_in_fov)}\")\n",
    "\n",
    "    # project points to image plane\n",
    "    xy_array = project_3d_to_2d_kannala(\n",
    "        points_in_fov,\n",
    "        calibs.cameras[camera].intrinsics[..., :3],\n",
    "        calibs.cameras[camera].distortion,\n",
    "    )\n",
    "\n",
    "    points = []\n",
    "    for i in range(xy_array.shape[0]):\n",
    "        x, y = int(xy_array[i, 0]), int(xy_array[i, 1])\n",
    "        cv2.circle(image, (x, y), 2, (255, 0, 0), -1)\n",
    "        points.append([x, y])\n",
    "\n",
    "    \"\"\"Draw a line in image.\"\"\"\n",
    "    def draw_line(image, line, color):\n",
    "        return cv2.polylines(\n",
    "            image.copy(),\n",
    "            [np.round(line).astype(np.int32)],\n",
    "            isClosed=False,\n",
    "            color=color,\n",
    "            thickness=10,\n",
    "        )\n",
    "\n",
    "    ground_truth_color = (19, 80, 41)\n",
    "    preds_color = (161, 65, 137)\n",
    "    image = draw_line(image, points, ground_truth_color)\n",
    "\n",
    "    # transform and draw predictions\n",
    "    if preds:\n",
    "        preds = reshape_ground_truth(preds)\n",
    "        print(f\"Number of pred points on image: {preds.shape[0]}\")\n",
    "        predpoints = transform_points(preds[:, :3], T_inv)\n",
    "        predpoints_in_fov = get_points_in_camera_fov(\n",
    "            calibs.cameras[camera].field_of_view, predpoints\n",
    "        )\n",
    "        xy_array_preds = project_3d_to_2d_kannala(\n",
    "            predpoints_in_fov,\n",
    "            calibs.cameras[camera].intrinsics[..., :3],\n",
    "            calibs.cameras[camera].distortion,\n",
    "        )\n",
    "        preds = []\n",
    "        for i in range(xy_array_preds.shape[0]):\n",
    "            x, y = int(xy_array_preds[i, 0]), int(xy_array_preds[i, 1])\n",
    "            cv2.circle(image, (x, y), 2, (255, 0, 0), -1)\n",
    "            preds.append([x, y])\n",
    "        image = draw_line(image, preds, preds_color)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879522e",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9efca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT_Model(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super(PT_Model, self).__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.is_pretrained = True\n",
    "\n",
    "        # freeze parameters and replace head\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.change_head_net()\n",
    "        self.loss_fn = torch.nn.L1Loss()\n",
    "        \n",
    "        # pytorch imagenet calculated mean/std\n",
    "        self.mean=[0.485, 0.456, 0.406]\n",
    "        self.std=[0.229, 0.224, 0.225]\n",
    "        \n",
    "        self.inter_train_outputs = []\n",
    "        self.inter_val_outputs = []\n",
    "        self.inter_test_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        mean = torch.tensor(self.mean).view(3, 1, 1).to(DEVICE)\n",
    "        std = torch.tensor(self.std).view(3, 1, 1).to(DEVICE)\n",
    "        \n",
    "        image = (image - mean) / std\n",
    "        label = self.model(image)\n",
    "        return label\n",
    "\n",
    "    def model_parameters(self):\n",
    "        return self.model.fc.parameters()\n",
    "\n",
    "    def change_head_net(self):\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        head_net = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1024, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, NUM_OUTPUT, bias=True),\n",
    "        )\n",
    "        self.model.fc = head_net\n",
    "\n",
    "    \n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        logits_label = self.forward(image)\n",
    "        logits_label = logits_label.unsqueeze(dim=1)\n",
    "        \n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_label, label)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": outputs[-1]['loss'],\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.shared_step(batch, \"train\")\n",
    "        self.inter_train_outputs.append(output)\n",
    "        return output\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.inter_train_outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.shared_step(batch, \"valid\")\n",
    "        self.inter_val_outputs.append(output)\n",
    "        return output\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.inter_val_outputs, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self.shared_step(batch, \"test\")\n",
    "        self.inter_test_outputs.append(output)\n",
    "        return output\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.inter_test_outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921a26a",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, nr_epochs=NUM_LOCAL_EPOCHS):\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator= 'gpu',\n",
    "        max_epochs=nr_epochs,\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model, \n",
    "        train_dataloaders=train_dataloader, \n",
    "        val_dataloaders=valid_dataloader,\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def validate(trainer, model, valid_dataloader):\n",
    "    valid_metrics = trainer.validate(model, dataloaders=valid_dataloader, verbose=False)\n",
    "    pprint(valid_metrics)\n",
    "\n",
    "\n",
    "def test(trainer, model, test_dataloader):\n",
    "    test_metrics = trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db59f4d",
   "metadata": {},
   "source": [
    "## supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32135d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_HP(dataset_root):\n",
    "    zod_frames = ZodFrames(dataset_root=dataset_root, version='full')\n",
    "    training_frames_all = zod_frames.get_split(constants.TRAIN)\n",
    "    validation_frames_all = zod_frames.get_split(constants.VAL)\n",
    "\n",
    "    return zod_frames, training_frames_all, validation_frames_all\n",
    "\n",
    "def is_valid(frame_id):\n",
    "    return frame_id not in UNUSED_FRAMES\n",
    "    \n",
    "zod_frames, training_frames_all, validation_frames_all = load_HP(DATASET_ROOT)\n",
    "\n",
    "training_frames = list(training_frames_all)[: int(len(training_frames_all) * SUBSET_FACTOR)]\n",
    "validation_frames = list(validation_frames_all)[: int(len(validation_frames_all) * SUBSET_FACTOR)]\n",
    "\n",
    "training_frames = [x for x in tqdm(training_frames) if is_valid(x)]\n",
    "validation_frames = [x for x in tqdm(validation_frames) if is_valid(x)]\n",
    "\n",
    "print(f'loaded {len(training_frames)} train frame ids.')\n",
    "print(f'loaded {len(validation_frames)} val frame ids.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loaders\n",
    "trainloaders, valloaders, testloader, completeTrainloader, completeValloader = ZODImporter(zod_frames=zod_frames,training_frames=training_frames,validation_frames=validation_frames).load_datasets(num_clients=1)\n",
    "\n",
    "# create model\n",
    "model = PT_Model()\n",
    "\n",
    "# train supervised\n",
    "trainer = train(model, completeTrainloader, completeValloader, nr_epochs=NUM_LOCAL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate \n",
    "validate(trainer, model, completeValloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test(trainer, model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86892859",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_HP_on_image(zod_frames, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251710a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 64em; }</style>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
